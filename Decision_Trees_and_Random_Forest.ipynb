{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vamshitn/Samsung-innovation-campus/blob/main/Decision_Trees_and_Random_Forest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq9GPMWyPlu3"
      },
      "source": [
        "# **Decision Trees and Random Forest**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ld2Xt3tXPq1W"
      },
      "source": [
        "## **Agenda**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAmn9_6uPswa"
      },
      "source": [
        "In this lesson, we will cover the following concepts of classification with the help of a business use case:\n",
        "* Decision Tree\n",
        "* Random Forest\n",
        "* Bagging and Bootstrapping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O-DvxyLCZjX"
      },
      "source": [
        "### **Decision Tree**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cq5DF2dzCbq7"
      },
      "source": [
        "Let us understand the basics of Decision Tree and its components with the help of a diagram.\n",
        "\n",
        "![DT1](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.8_Decision_Trees_and_Random_Forest/Trainer_PPT_and_IPYNB/DT1.JPG)\n",
        "\n",
        "\n",
        "* Decision tree is a tree-like structure in which the internal node represents the test on an attribute.\n",
        "\n",
        "* The topmost node in a decision tree is known as the root node. It learns to partition based on the attribute value.\n",
        "\n",
        "* Each branch represents the outcome of the test and each leaf node represents the class label.\n",
        "\n",
        "* A path from root to leaf represents the classification rules."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0oo4gMSLZ8Y"
      },
      "source": [
        "#### **Splitting:**\n",
        "\n",
        "* If pure node is detected, it can be classified as leaf node.\n",
        "* If impure node is detected, the best attribute is selected and splitting is continued until a pure node is detected.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aXpv-sZMtiL"
      },
      "source": [
        "#### **Example:**\n",
        "\n",
        "Form a decision tree to check if the match will be played or not based on climatic conditions\n",
        "\n",
        "<br>\n",
        "\n",
        "![DT2](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.8_Decision_Trees_and_Random_Forest/Trainer_PPT_and_IPYNB/DT2.JPG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab55LVWMkFsg"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "#### **1. Entropy and Information Gain**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0GJz_PYWpkE"
      },
      "source": [
        "  ![DT3](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.8_Decision_Trees_and_Random_Forest/Trainer_PPT_and_IPYNB/DT3.JPG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhgGQm-kkJWR"
      },
      "source": [
        "\n",
        "\n",
        "Let us consider an example where:\n",
        "* Sample for humidity = [High, Normal] = [9+, 5-]\n",
        "\n",
        "* Sample for wind = [Weak, Strong] = [9+,5-]\n",
        "<br><br>\n",
        "\n",
        "**a. Calculate gain:**\n",
        "\n",
        "  ![DT4](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.8_Decision_Trees_and_Random_Forest/Trainer_PPT_and_IPYNB/DT4.JPG)\n",
        "\n",
        "\n",
        "**b. Select highest gain:**\n",
        "\n",
        "  ![DT5](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.8_Decision_Trees_and_Random_Forest/Trainer_PPT_and_IPYNB/DT5.JPG)\n",
        "\n",
        "**c. Inference:**\n",
        "\n",
        "* Humidity provides the best prediction for the target\n",
        "* For each possible value of Humidity, you can add a successor to the tree\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsjD3dJIo2tW"
      },
      "source": [
        "#### **Overfitting and Pruning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi0K3A_Uo844"
      },
      "source": [
        "##### **Overfitting:**\n",
        "\n",
        "Overfitting refers to the unwanted behavior of a machine learning algorithm. It occurs when a model's performance on the training dataset improves at the expense of poor performance on non-training data, such as a holdout test dataset or new data. By testing a machine learning model on both the training dataset and a hold-out dataset, we can determine whether a machine learning model is overfitted. If the performance of the model on the training dataset is significantly better than the performance on the test dataset, then the model may have overfitted the training dataset.\n",
        "\n",
        "---\n",
        "  ![DT6](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.8_Decision_Trees_and_Random_Forest/Trainer_PPT_and_IPYNB/DT6.JPG)\n",
        "\n",
        "\n",
        "##### **Pruning:**\n",
        "\n",
        "* The removal of the sub-tree at a node is called pruning.\n",
        "* It is performed iteratively until the tree shows maximum accuracy.\n",
        "* There are two ways to perform pruning:\n",
        "  * Stop the development of the tree at an early stage, i.e., before perfect classification\n",
        "\n",
        "  * Allow overfitting and later prune the tree\n",
        "\n",
        "  ![DT7](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.8_Decision_Trees_and_Random_Forest/Trainer_PPT_and_IPYNB/DT7.JPG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDV5EPWlRcCf"
      },
      "source": [
        "#### **2. Gini Index**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ib-JzO98SzGo"
      },
      "source": [
        "The Gini Index is a metric that measures the likelihood that a randomly chosen element will be incorrectly identified. It means an attribute with a low Gini index should be preferred. The decision tree algorithm aims to achieve partitions in the terminal nodes as pure as possible. The Gini index is one method used to achieve this.\n",
        "* It is calculated by subtracting the sum of the squared probabilities of each class from one.\n",
        "\n",
        "\n",
        " ##### **Formula:**\n",
        "$$Gini(D)=1-\\sum_{i=1}^{C}\\ (p_i)^2$$\n",
        "\n",
        "  Where, $p_i$ is the probability that a tuple in $D$ belongs to class $C_i$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAQDBObuaNK6"
      },
      "source": [
        "* It uses a binary split for every variable where, weighted sum of the impurity of each partition is computed in splitting.\n",
        "  \n",
        "  ##### **Example:**\n",
        "  \n",
        "  Binary split on variable A partitions data D into D1 and D2.\n",
        "\n",
        "  What is the Gini index of D?\n",
        "\n",
        "  $$Gini_A(D)= \\frac{|D_1|}{|D|}\\ Gini(D_1)\\ + \\frac{|D_2|}{|D|}\\ Gini(D_2)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIumnWfkf3Kw"
      },
      "source": [
        "* In case of a discrete-valued attribute, the subset that gives the minimum gini index is selected as a splitting attribute. In the case of continuous-valued attributes, the strategy is to select each pair of adjacent values as a possible split-point and point with smaller gini index chosen as the splitting point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCke-OVQaNVD"
      },
      "source": [
        "##### **Features:**\n",
        "\n",
        "* Gini index favors larger partitions.\n",
        "* It uses the squared proportion of the classes.\n",
        "* Gini index is perfectly classified when it is zero."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyxUCNuhgaWG"
      },
      "source": [
        "### **Random Forest**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7baQuXrMoDTA"
      },
      "source": [
        "Random Forests is a supervised machine learning algorithm used in classification and regression problems. It builds decision trees for different samples and takes the majority for classification and the average for regression.\n",
        "\n",
        "##### **Features:**\n",
        "* Runs efficiently on larger databases\n",
        "* Handles thousands of input variables without variable deletion\n",
        "* Provides an estimate of what variables are important in the classification\n",
        "* Provides a method to detect variable interactions experimentally\n",
        "\n",
        "Let us extrapolate from the previous topic and discuss the concepts of random forest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUSSwa2koeh7"
      },
      "source": [
        "### **Bagging and Bootstrapping**\n",
        "\n",
        "![RF1](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.8_Decision_Trees_and_Random_Forest/Trainer_PPT_and_IPYNB/RF1.JPG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvA7U-BUozvM"
      },
      "source": [
        "#### **Road to random forest classifier:**\n",
        "\n",
        "**1. Create bootstrap samples from the training data**\n",
        "\n",
        "  ![RF2](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.8_Decision_Trees_and_Random_Forest/Trainer_PPT_and_IPYNB/RF2.JPG)\n",
        "\n",
        "\n",
        "**2. Each sample contributes to a decision tree classifier**\n",
        "\n",
        "  ![RF3](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.8_Decision_Trees_and_Random_Forest/Trainer_PPT_and_IPYNB/RF3.JPG)\n",
        "\n",
        "\n",
        "**3. Majority vote is taken by using the various trees generated from various bootstrap samples**\n",
        "\n",
        "  ![RF4](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.8_Decision_Trees_and_Random_Forest/Trainer_PPT_and_IPYNB/RF4.JPG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wnm4ydz6OzP"
      },
      "source": [
        "## **Use Case: Decision Tree vs. Random Forest**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZU-gSMTb6ywv"
      },
      "source": [
        "**Problem statement:**\n",
        "PeerLoanKart is an NBFC (non-banking financial company) that facilitates peer-to-peer loans.\n",
        "It connects people who need money (borrowers) with people who have money (investors). As an investor, you would want to invest in people who showed a profile of having a high probability of paying you back.\n",
        "Create a model that will help predict whether a borrower will repay the loan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0GCt1xb66-B"
      },
      "source": [
        "**Analysis to be done:** Increase profits by up to 20% as NPAs will be reduced due to loan disbursal to creditworthy borrowers only"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxZ2BD9x7LYx"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbBNBCkv7XUs"
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWKdMSTx8LJw"
      },
      "source": [
        "### Import Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "TTrdaCi4FHJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source https://raw.githubusercontent.com/JangirSumit/data_science/master/26th%20May%20Assignments/case%20study%203/loan_borowwer_data.csv"
      ],
      "metadata": {
        "id": "ydOxw5WuoHgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datalink = 'https://raw.githubusercontent.com/JangirSumit/data_science/master/26th%20May%20Assignments/case%20study%203/loan_borowwer_data.csv'"
      ],
      "metadata": {
        "id": "Ro2XRdByE2RD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwVWsZXh8oNo"
      },
      "outputs": [],
      "source": [
        "# Importing the dataset\n",
        "df = pd.read_csv(datalink,delimiter=',')\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C09y9uHi88us"
      },
      "source": [
        "### Exploratory Data Analysis\n",
        "Create a histogram of two FICO distributions on top of each other, one for each credit.policy outcome.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYHyDLHI9Hbs"
      },
      "outputs": [],
      "source": [
        "# Creating a histogram\n",
        "plt.figure(figsize=(10,6))\n",
        "df[df['credit.policy']==1]['fico'].hist(alpha=0.5,color='blue',  bins=30,label='Credit.Policy=1')\n",
        "df[df['credit.policy']==0]['fico'].hist(alpha=0.5,color='red', bins=30,label='Credit.Policy=0')\n",
        "plt.legend()\n",
        "plt.xlabel('FICO')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vADn2jkA9ZM1"
      },
      "source": [
        "Create a similar figure; select the **not.fully.paid** column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43XMPQrM9Vpx"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "df[df['not.fully.paid']==1]['fico'].hist(alpha=0.5,color='blue', bins=30,label='not.fully.paid=1')\n",
        "df[df['not.fully.paid']==0]['fico'].hist(alpha=0.5,color='red', bins=30,label='not.fully.paid=0')\n",
        "plt.legend()\n",
        "plt.xlabel('FICO')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKkexen19nnp"
      },
      "source": [
        "Create a countplot using seaborn showing the counts of loans by purpose with the hue defined by **not.fully.paid**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nX0tpWwN9XAd"
      },
      "outputs": [],
      "source": [
        "# Creating countplot\n",
        "plt.figure(figsize=(11,7))\n",
        "sns.countplot(x='purpose',hue='not.fully.paid',data=df,palette='Set1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBeRLsxn91KH"
      },
      "source": [
        "### Setting Up the Data\n",
        "Create a list of elements containing the string **purpose**. Name this list **cat_feats**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xb9EsE3I957j"
      },
      "outputs": [],
      "source": [
        "cat_feats = ['purpose']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4R80ZbyA-EAw"
      },
      "source": [
        "Now use **pd.get_dummies (loans,columns=cat_feats,drop_first=True)** to create a fixed and larger data frame that has new feature columns with dummy variables. Name this data frame **final_data**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lE2xPGxK-E_J"
      },
      "outputs": [],
      "source": [
        "final_data = pd.get_dummies(df,columns=cat_feats,drop_first=True)\n",
        "final_data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7B7ym3hj-QQi"
      },
      "source": [
        "### Train-Test Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agWHMsnTNV-n"
      },
      "source": [
        "Let us distribute the data into **training** and **test** datasets using the **train_test_split()** function."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "wWJnsICrF0aV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJFThy0M-SH5"
      },
      "outputs": [],
      "source": [
        "X = final_data.drop('not.fully.paid',axis=1)\n",
        "y = final_data['not.fully.paid']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5CsgVHo-ZiY"
      },
      "source": [
        "### Training Decision Tree Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_5JMRwWNV-p"
      },
      "source": [
        "Let us create a **DecisionTreeClassifier** from **sklearn.tree**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdXV-5ug-bnt"
      },
      "outputs": [],
      "source": [
        "# Decision tree model\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dtree = DecisionTreeClassifier()\n",
        "dtree.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3rgtFPt-jDp"
      },
      "source": [
        "### Evaluating Decision Tree\n",
        "Let us create a classification report for the **DecisionTreeClassifier** we created earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4VXtpcf-uj5"
      },
      "outputs": [],
      "source": [
        "# Classification report\n",
        "predictions = dtree.predict(X_test)\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "print(classification_report(y_test,predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9jyUo3s-90e"
      },
      "source": [
        "### Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GDKF3cC-_ck"
      },
      "outputs": [],
      "source": [
        "# Creating a confusion matrix\n",
        "print(confusion_matrix(y_test,predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3iyLsJx_Jxn"
      },
      "source": [
        "### Training Random Forest Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOupjIRWNV-3"
      },
      "source": [
        "Let us create a **RandomForestClassifier** for **sklearn.ensemble** library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRSOGZLn_Of6"
      },
      "outputs": [],
      "source": [
        "# Random forest model\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rfc = RandomForestClassifier(n_estimators=600)\n",
        "rfc.fit(X_train,y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A58g2fMn_Zd0"
      },
      "source": [
        "### Evaluating Random Forest Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIJaPQlpNV-6"
      },
      "source": [
        "Let us create a classification report for the **RandomForestClassifier** we created earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gI9TaZW4_a71"
      },
      "outputs": [],
      "source": [
        "predictions = rfc.predict(X_test)\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "print(classification_report(y_test,predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzA1S5HJ_iuq"
      },
      "source": [
        "### Printing the Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmsxu1_F_o7C"
      },
      "outputs": [],
      "source": [
        "print(confusion_matrix(y_test,predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtqpkqSUAmK2"
      },
      "source": [
        "### **Conclusion**\n",
        "From the above use case, we can see that the accuracy of random forest is better than decision tree."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}